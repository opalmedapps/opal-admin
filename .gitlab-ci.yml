# Change pip's cache directory to be inside the project directory since we can
# only cache local items.
# see: https://docs.gitlab.com/ee/ci/caching/#cache-python-dependencies
variables:
  PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"
  MARIADB_ROOT_PASSWORD: $DB_ROOT_PASSWORD
  # ensure that user has permissions for test DB to be used by pytest
  MARIADB_DATABASE: test_$DB_NAME
  MARIADB_USER: $DB_USER
  MARIADB_PASSWORD: $DB_PASSWORD
  # collapsible multi-line commands
  FF_SCRIPT_SECTIONS: "true"

default:
  image: python:3.11.9-alpine3.19
  # mark all jobs interruptible by default
  # https://docs.gitlab.com/ee/ci/yaml/index.html#workflowauto_cancelon_new_commit
  interruptible: true

.uses_python_dependencies:
  needs:
    - job: setup
  before_script:
    - uname -a
    - python --version
    - source .venv/bin/activate

.uses_django:
  extends:
    - .uses_python_dependencies
  before_script:
    - !reference [.uses_python_dependencies, before_script]
    # install dependencies for mysqlclient
    - apk add --no-cache build-base mariadb-dev
    # set up env file for DB service
    # use sample env file
    - cp .env.sample .env
    - sed -i "s/^DATABASE_HOST=.*/DATABASE_HOST=mariadb/" .env
    - sed -i "s/^DATABASE_NAME=.*/DATABASE_NAME=$DB_NAME/" .env
    - sed -i "s/^DATABASE_USER=.*/DATABASE_USER=$DB_USER/" .env
    - sed -i "s/^DATABASE_PASSWORD=.*/DATABASE_PASSWORD=$DB_PASSWORD/" .env
    # set up legacy DB connection
    # reuse the same database as for the other tests to make it easier
    - sed -i "s/^LEGACY_DATABASE_HOST=.*/LEGACY_DATABASE_HOST=mariadb/" .env
    - sed -i "s/^LEGACY_DATABASE_PORT=.*/LEGACY_DATABASE_PORT=3306/" .env
    - sed -i "s/^LEGACY_DATABASE_NAME=.*/LEGACY_DATABASE_NAME=OpalDB/" .env
    - sed -i "s/^LEGACY_DATABASE_USER=.*/LEGACY_DATABASE_USER=$DB_USER/" .env
    - sed -i "s/^LEGACY_DATABASE_PASSWORD=.*/LEGACY_DATABASE_PASSWORD=$DB_PASSWORD/" .env
    # set up QuestionnaireDB connection as well
    - sed -i "s/^LEGACY_QUESTIONNAIRE_DATABASE_NAME=.*/LEGACY_QUESTIONNAIRE_DATABASE_NAME=QuestionnaireDB/" .env
    # generate secret key
    - SECRET_KEY=$(python -c "import secrets; print(secrets.token_urlsafe())")
    - sed -i "s/^SECRET_KEY=.*/SECRET_KEY=$SECRET_KEY/" .env


stages:
  - static analysis
  - checks
  - build
  - test
  - deploy

setup:
  stage: .pre
  before_script:
    # install dependencies for mysqlclient
    - apk add --no-cache build-base mariadb-dev
  script:
    - pip install uv
    - uv venv
    - source .venv/bin/activate
    - uv pip install --cache-dir .cache/uv -r requirements/development.txt
  artifacts:
    # pin artifacts to the branch
    # see: https://docs.gitlab.com/ee/ci/pipelines/job_artifacts.html#use-cicd-variables-to-define-the-artifacts-name
    name: $CI_COMMIT_REF_SLUG
    expire_in: "1 day"
    paths:
      - .venv
    # don't cache since it takes just as long to upload and download the cache than to re-download dependencies with uv


flake8:
  stage: static analysis
  extends:
    - .uses_python_dependencies
  script:
    - flake8 --version
    # install plugin to produce code climate report
    - pip install uv
    - uv pip install flake8-gl-codeclimate
    # don't fail if there are errors so the result can also be printed to the console for logs
    - flake8 --format gl-codeclimate --output-file gl-code-quality-report.json || true
    # fail if JSON file does not contain empty array
    - cat gl-code-quality-report.json | grep -q "\[\]"
  artifacts:
    when: always
    reports:
      codequality: gl-code-quality-report.json

mypy:
  stage: static analysis
  extends:
    - .uses_django
  script:
    # install plugin to produce code climate report
    - pip install uv
    - uv pip install mypy-gitlab-code-quality
    - mypy --version
    # export output and don't fail if there are errors
    - mypy config/ opal/ --no-error-summary > mypy-out.txt || true
    - PYTHONHASHSEED=0 mypy-gitlab-code-quality < mypy-out.txt > codequality.json
     # fail if JSON file does not contain empty array
    - cat codequality.json | grep -q "\[\]"
  artifacts:
    when: always
    reports:
      codequality: codequality.json

markdownlint:
  stage: static analysis
  image:
    name: davidanson/markdownlint-cli2:v0.13.0
    # overwrite default entrypoint (which is a call to markdownlint-cli2)
    entrypoint: [""]
  # does not need setup artifacts
  needs: []
  script:
    - markdownlint-cli2 --version
    # use the config file that is stored outside the root
    - markdownlint-cli2 --config .gitlab/markdownlint/.markdownlint-cli2.yaml "**/*.md" "#.venv"
  artifacts:
    when: always
    reports:
      codequality: markdownlint-cli2-codequality.json

pytest:
  stage: test
  # start this job as early as possible as it is long-running
  needs:
    - job: setup
  extends:
    - .uses_django
  # set up DB service only for the test job since it is not used by others
  services:
    - mariadb:10.11.8-jammy
  script:
    # create additional DBs for legacy DB tests (OpalDB & QuestionnaireDB)
    - apk add --no-cache mariadb-client
    - MYSQL_PWD=$DB_ROOT_PASSWORD mysql -uroot -hmariadb -e "GRANT ALL PRIVILEGES ON \`test_OpalDB\`.* TO \`$MARIADB_USER\`@\`%\`;"
    - MYSQL_PWD=$DB_ROOT_PASSWORD mysql -uroot -hmariadb -e "GRANT ALL PRIVILEGES ON \`test_QuestionnaireDB\`.* TO \`$MARIADB_USER\`@\`%\`;"
    - pytest --version
    # -m "" runs all tests, even the ones marked as slow
    - coverage run -m pytest -m "" --junitxml=report.xml
  artifacts:
    when: always
    reports:
      junit: report.xml
    paths:
      - .coverage

coverage:
  stage: test
  extends:
    - .uses_python_dependencies
  script:
    # allow XML creation to fail to be able to parse coverage out of text report
    - coverage xml || true
    - coverage report
  needs:
    - job: setup
    - job: pytest
  coverage: '/^TOTAL.+?(\d+.\d+\%)$/'
  # min coverage is set to 100, allow job to fail in order to signal that the coverage dropped
  # it does not mean it always has to be fixed
  allow_failure: true
  artifacts:
    when: always
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage.xml

django migrations:
  stage: checks
  extends:
    - .uses_django
  script:
    # Missing migrations
    - echo "Checking that there are no model changes for which there is no migration..."
    - python manage.py makemigrations --dry-run --check

django translations:
  stage: checks
  extends:
    - .uses_django
  script:
    # install dependencies for git diff
    - apk add --no-cache gettext git
    # Missing translation file updates
    - echo "Checking that translation files have been updated ..."
    - python manage.py makemessages --add-location file -l fr
    # due to git LFS the LFS-tracked files show up in the diff, limit the diff to only .po files
    - git diff --ignore-matching-lines=POT-Creation-Date --exit-code *.po

django translations fuzzy:
  stage: checks
  needs: []
  script:
    - apk add --no-cache grep
    # Ensure there are no fuzzy translation strings
    - echo "Checking for presence of fuzzy translation strings (need updates) ..."
    - FUZZY_STRINGS=$(grep --include=\*.po -rnw './opal' -e "#, fuzzy" -B01 -A02 | grep -E '#[~]?\| msgid' -B02 -A01 || true)
    - >
        if [[ $FUZZY_STRINGS ]]; then
          echo "$FUZZY_STRINGS"
          (! echo "$FUZZY_STRINGS" | grep -q .)
        fi


django translations commented:
  stage: checks
  needs: []
  script:
    - apk add --no-cache grep
    # Ensure there are no commented out translation strings
    - echo "Checking for presence of commented out translation strings ..."
    - COMMENTED_STRINGS=$(grep --include=\*.po -rnw './opal' -e "#~ msgid" -B01 -A03 || true)
    - >
        if [[ $COMMENTED_STRINGS ]]; then
          echo "$COMMENTED_STRINGS"
          (! echo "$COMMENTED_STRINGS" | grep -q .)
        fi

django auto-named migrations:
  stage: checks
  needs: []
  script:
    # Prevent auto-named migrations
    - echo "Checking that migrations are not auto-named ..."
    # See: https://adamj.eu/tech/2020/02/24/how-to-disallow-auto-named-django-migrations/
    # Return exit code of 1 if results found, 0 otherwise
    # See: https://serverfault.com/q/225798 and https://stackoverflow.com/a/53753605
    - (! find ./opal -regex '.*/migrations/.*_auto_.*\.py$' | grep -q .)

django checks:
  stage: checks
  extends:
    - .uses_django
  script:
    # Run checks to be sure we follow all django's best practices:
    - echo "Checking that all Django checks pass ..."
    - python manage.py check --fail-level WARNING

django template validation:
  stage: checks
  extends:
    - .uses_django
  script:
    # Check templates
    - echo "Validating templates ..."
    - python manage.py validate_templates

check crontab:
  stage: checks
  extends:
    - .uses_django
  script:
    # Check that the crontab references the correct commands and that they execute
    - scripts/check_crontab.sh

build image:
  stage: build
  image: docker:26.1.3
  needs: []
  services:
    - docker:26.1.3-dind
  variables:
    # Use TLS https://docs.gitlab.com/ee/ci/docker/using_docker_build.html#tls-enabled
    DOCKER_HOST: tcp://docker:2376
    DOCKER_TLS_CERTDIR: "/certs"
  before_script:
    # login to CI registry: https://docs.gitlab.com/ee/ci/docker/using_docker_build.html#option-1-run-docker-login
    - echo $CI_REGISTRY_PASSWORD | docker login $CI_REGISTRY --username $CI_REGISTRY_USER --password-stdin
  script:
    - export IMAGE_REPOSITORY=$CI_REGISTRY_IMAGE
    # create an image tagged with the branch or tag name
    - export IMAGE_BRANCH=$IMAGE_REPOSITORY:$CI_COMMIT_REF_SLUG
    # plus for the default branch create a latest and commit tag
    # the commit tag allows us to get images of previous commits in case of problems
    - export IMAGE_LATEST=$IMAGE_REPOSITORY:latest
    - export IMAGE_COMMIT=$IMAGE_REPOSITORY:$CI_COMMIT_SHA
    # make use of the Docker cache to speed up building
    # requires a specific builder (docker-container) to cache to the registry cache to support multi-stage builds
    # https://docs.docker.com/build/drivers/
    # https://docs.docker.com/build/cache/backends/registry/
    # only cache to build cache if on main
    # the image for a branch/MR needs to be pushed so that container scanning can scan it
    - export IMAGE_BUILD_CACHE=$IMAGE_REPOSITORY/docker-build-cache:latest
    - |
      if [[ "$CI_COMMIT_BRANCH" == "$CI_DEFAULT_BRANCH" ]]; then
        echo "Running on default branch. Pushing latest and commit tags, and caching to registry cache..."
        docker context create builder
        docker buildx create builder --use
        docker buildx build --push --cache-to type=registry,ref=$IMAGE_BUILD_CACHE,mode=max --cache-from type=registry,ref=$IMAGE_LATEST --cache-from type=registry,ref=$IMAGE_BUILD_CACHE --tag $IMAGE_BRANCH --tag $IMAGE_LATEST --tag $IMAGE_COMMIT .
      else
        echo "Running on branch..."
        docker buildx build --push --cache-from type=registry,ref=$IMAGE_LATEST --cache-from type=registry,ref=$IMAGE_BUILD_CACHE --tag $IMAGE_BRANCH .
      fi

# Try building the documentation site to ensure there are no issues
build docs:
  stage: build
  needs:
    - job: setup
    - job: markdownlint
  extends:
   - .uses_django
  script:
    - pip install uv
    - uv pip install -r requirements/docs.txt
    - mkdocs --version
    # abort on warnings to catch invalid cross-references
    - mkdocs build --strict

# Publish docs to GitLab Pages
# see: https://squidfunk.github.io/mkdocs-material/publishing-your-site/#gitlab-pages
#
pages:
  stage: deploy
  only:
    - main
  extends:
    - .uses_django
  script:
    - pip install uv
    - uv pip install -r requirements/docs.txt
    - mkdocs build --site-dir public
  artifacts:
    paths:
      - public
  needs:
    - setup
    - build docs


workflow:
  auto_cancel:
    on_new_commit: interruptible


include:
  # run pipelines for default branch, tags, and all types of merge request pipelines
  # to support merge trains
  # see: https://docs.gitlab.com/ee/ci/pipelines/merge_trains.html#enable-merge-trains
  # https://gitlab.com/gitlab-org/gitlab/-/blob/master/lib/gitlab/ci/templates/Workflows/MergeRequest-Pipelines.gitlab-ci.yml
  - template: 'Workflows/MergeRequest-Pipelines.gitlab-ci.yml'
  # use latest template versions to run security scanning jobs also in merge request pipelines:
  # https://docs.gitlab.com/ee/user/application_security/index.html#use-security-scanning-tools-with-merge-request-pipelines
  # Secret Detection: https://docs.gitlab.com/ee/user/application_security/secret_detection/
  # https://gitlab.com/gitlab-org/gitlab/-/blob/master/lib/gitlab/ci/templates/Jobs/Secret-Detection.latest.gitlab-ci.yml
  - template: Jobs/Secret-Detection.latest.gitlab-ci.yml
  # Dependency Scanning: https://docs.gitlab.com/ee/user/application_security/dependency_scanning/
  # https://gitlab.com/gitlab-org/gitlab/-/blob/master/lib/gitlab/ci/templates/Jobs/Dependency-Scanning.latest.gitlab-ci.yml
  - template: Jobs/Dependency-Scanning.latest.gitlab-ci.yml
  # Container Scanning: https://docs.gitlab.com/ee/user/application_security/container_scanning/
  # https://gitlab.com/gitlab-org/gitlab/-/blob/master/lib/gitlab/ci/templates/Jobs/Container-Scanning.latest.gitlab-ci.yml
  - template: Jobs/Container-Scanning.latest.gitlab-ci.yml
  # SAST Scanning: https://docs.gitlab.com/ee/user/application_security/sast/
  # https://gitlab.com/gitlab-org/gitlab/-/blob/master/lib/gitlab/ci/templates/Jobs/SAST.latest.gitlab-ci.yml
  - template: Jobs/SAST.latest.gitlab-ci.yml

# Customizations
#

# Dependency Scanning
# https://docs.gitlab.com/ee/user/application_security/dependency_scanning/#customizing-analyzer-behavior
gemnasium-dependency_scanning:
  variables:
    # ensure that the JS lockfile gets detected in opal/static/thirdparty
    DS_MAX_DEPTH: 3

gemnasium-python-dependency_scanning:
  variables:
    # enforce running this without the "exists: **/requirements.txt" rule
    PIP_REQUIREMENTS_FILE: "requirements.txt"
  # the scanner actually installs the dependencies via pip and therefore requires the mysql client to be installed
  before_script:
    # install dependencies for mysqlclient
    - apt update && apt-get install -y libmariadb-dev pkg-config
    # prepare a single requirements file
    # prepare a single requirements.txt that contains all relevant dependencies
    - mv requirements/base.txt requirements.txt
    # remove the first line ("-r ...")
    - tail -n +2 requirements/development.txt >> requirements.txt
    - tail -n +2 requirements/production.txt >> requirements.txt
    - cat requirements.txt

# Container Scanning
# https://docs.gitlab.com/ee/user/application_security/container_scanning/#customizing-the-container-scanning-settings
container_scanning:
  needs:
    - build image
  variables:
    CS_IMAGE: $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_SLUG

# Secret Detection
# https://docs.gitlab.com/ee/user/application_security/secret_detection/pipeline/index.html#overriding-the-analyzer-jobs
secret_detection:
  # don't get artifacts from setup job
  needs: []
